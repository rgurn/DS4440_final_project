<!doctype html>
<html lang="en">
<head>
<title>Analysis of GANs</title>
<meta property="og:title" content=Analysis of GANs" />
<meta name="twitter:title" content="Analysis of GANs" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Analysis of GANs</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks"</h2>
<p> By: Riya Gurnani and Xinyu Wu
<p> Paper Link: <a href = "https://openreview.net/pdf?id=Hyg_X2C5FX">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</a></p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
 <p>
  In this project, we delve into Generative Adversarial Networks (GANs) to better visualize and understand them. <q>GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</q> by Bau, Strobelt, et al. (2019) introduces a framework for visualizing and understanding GANs at multiple levels of abstraction. The paper provides insights into how GANs represent and generate visual context internally which is crucial given their widespread use in real-world applications. We aim to better understand and expand upon the framework from the paper. Gaining deeper insights into these models will allow for improved performance. Our goal is to successfully implement parts of the code from the paper and test the framework on new objects to evaluate the behavior of the model across various images. This will also provide further insight into their generalization capabilities. For example, the interactive demo from the paper focused on different church images and the ability to remove/add various objects to the image such as trees, grass, and door. We want to expand on this to test with different types of images (conference room, restaurant, and dining room) and new objects not previously tested (window and chair) to understand how well we can apply this concept to find the highest activating units for those objects and be able to generate them in new images.
 </p>

<h2>Paper Review</h2>
<p>
 While Generative Adversarial Networks (GANs) have shown significant ability to produce realistic images, there is still a gap in understanding how different knowledge and variants affect model performance. Therefore, this paper by Bau et al. looks at the internal representations of GANs and how they are structured, including neurons, objects, and contextual relationships between objects (Bau, Strobelt, et al., 2019). 
 Objects that are used in this paper include trees, grass, doors, skies, clouds, bricks, and domes. To examine these objects, the paper utilizes representations to denote tensor outputs (r) on each layer of the generator (G) which generates an image (x) through z layers. In relation to units r, the paper understands it in two phases: (1) dissection, (2) intervention. Dissection involves identifying classes that are explicitly represented in r and measuring the relationships between each single unit and their class, c. The latter, intervention, uses the classes found through dissecting r in order to analyze causal relationships between units and object classes by turning on and off various sets of units. 
</p>
<p>
 Building on previous literature, the paper uses an intersection-over-union measure that quantitatively measures the spatial agreement between a unit&#39;s threshold featuremap and a concept&#39;s segmentation. Therefore, this metric is able to rank and pair units with the concepts that most closely match. However, outputs generally depend on multiple parts of the representation, and a combination of units must be identified in relation to a specific object. Thus, an ablation and insertion to the decomposed images can help identify the causal relationship between units and classes, or the average causal effect. This effect can be optimized using L2 regularization and stochastic gradient descent to achieve stronger causal effect.
 The paper has a number of key findings when understanding units between different datasets, layers, and models: (1) individual unit object detectors form, (2) there are interpretable units for different scene categories such as living room and kitchen, (3) different layers have varying interpretable units, and (4) these interpretable units can help differentiate between GAN models. For the purposes of our project, our goal is to identify these interpretable units of objects not tested in this paper, such as windows and chairs, in hopes of replicating similar results. Overall, the paper provides a strong basis to understanding neurons, objects, and their relationships in GAN models by dissecting and intervening with individual layers. 
</p>
 
<h2>Method</h2>
<p> Our Implementation Link: <a href = "https://colab.research.google.com/drive/10UPKkcwzi6PX9RKPq7oID2sOQPjAoJti?usp=sharing">DS4440 Final Project</a></p>
<p>
 In order to achieve our goal of testing different objects, our first step was to understand the complex framework detailed in the paper and its provided code. The code from the paper analyzes how objects are encoded within a GAN generator through dissection and intervention. By intervening in the network and observing the effects on generated images, we gain insights into the causal relationships between units and objects. Due to more limited GPU power and some dependency issues, we did not use the entire code from the paper github. Instead, we examined how they created their interactive demo and tutorials. These pieces of code included the highest activated units for the trees. However, for the purposes of our project, we needed to find the highest activating units for our chosen objectsâ€”windows and chairs.
</p>
<p>
 Another piece of code from the paper has the functionality to load an image, click and drag on it to select a certain part in order to trigger either an erasing or adding action. Thus, to find the highest activating units for our own objects, we utilized parts of this code and added onto it. We wanted to include a variety of testing so we loaded different GAN generators for the objects. For reference, here is the first five layers of the pretrained GAN model used that is then trained on the different image settings:
</p>
 <center><img src="images/pretrained_GAN_sample.png" width="400px" height="500px"></center>
 <p>
  For window, we used <q>church</q> and <q>conference room</q> and for chair, we used <q>restaurant</q> and <q>dining room</q>. From here, we were able to create a segment of code to randomly choose one of the generated images and used the select feature tool to select that object in the image and display the highest activating units. We created a script to run this for 50 randomly selected images for each object and setting (window in church, window in conference room, chair in restaurant, and chair in dining room). Here is an example of what it looks like to find the selected units where the window is selected in the conference room:
</p>
<center><img src="images/get_units_test.png" width="600px" height="300px"></center>
 <p>
  We saved all of those selected units and images numbers to four files corresponding to each object-setting combination. Then, we did some analysis on this data to find the ten most commonly occurring units for each of the four object-setting combinations. Therefore, we had four sets of units across the two objects. Using these most commonly activated units, we then ran testing to see how well those units were selecting the objects and if they could visualize the selected objects in new images and even generate the object when we run the select tool over a part of the image. 
 </p>

<h2>Findings</h2>
<p>
 For our testing on how well the chosen units recognize and generate the object in new images, we followed similar processes from the paper for how they tested tree generation in the church images (amongst other objects). We repeated the same process across all four object-setting combinations, with windows generally having higher performance due to a number of factors discussed in this section. This process involved using the highest activating units found from our object identification methodology and running our program to highlight places in the image in order to insert the given object. From here, we recorded the resulting image in order to identify any visible changes and objects created. 
</p>
<p>
 For our first object, windows, adding the object to church images led to varied results. Below is a visualization of before and after we tried to generate window objects on ten different church images. There are a few highlighted sections (in red) in the fourth and sixth images that show a generation. For the other images, we found that trying to generate would sometimes blur parts of the image but not generate a window. In the first, seventh, and ninth images, we noticed slight changes but no objects generated. Rather, we found that for those particular images, if we selected a segment near an existing window, it would slightly expand it and make the window larger.
</p>
<center><img src="images/window_church.png" width="900px" height="350px"></center>
 <p>
  Overall, our chosen units did not generate what we had aimed for. We believe that this is largely in part to the fact that there are many small windows on churches that are difficult to recognize, even for our human eyes, so it can be hard for the model to distinguish these areas. In order to further our understanding of the performance of the model, we also printed out unit masks from the top activating images for the highest activating unit for windows in the church images, shown below. The visualization shows some success for unit 221. More unit testing can be found in our implementation link:
 </p>
 <center><img src="images/window_church_unit.png" width="900px" height="350px"></center>
 <p>
  We followed a similar testing process for window objects in the conference room images. We had more success in terms of generating windows in this context, likely due to the larger window sizes in these images as well as the greater contrast in brightness between the windows and the indoor conference rooms. In the visualization below, there are ten images with the before and after for when we selected different parts of the image. There are highlighted sections (in red) for the first four images as well as the last two images. In those tests, we saw object generation in the form of a brighter light forming as a window object or a white canvas. In the other images, we saw small changes but no significant object generation.
 </p>
 <center><img src="images/window_conference.png" width="900px" height="350px"></center>
 
<h2>Conclusion</h2>
<p>
 While our model results were not as high-performing as the original paper, the tests that we conducted on the four settings allowed us to identify a number of learnings and implications. We found that certain images have qualities that make it easier to identify activation units, such as higher brightness, greater contrast relative to surrounding objects, more consistency in color, and less variability in shapes. Thus, we believe that objects such as trees, which generally have the same color and similar shapes and less variability, are easier to generate than images with high variability such as chairs. Future work in this area can expand to include other types of image settings and objects. This could include erasing or adding facial features from images of human faces, or being able to erase and add humans or animals to different settings. Further work on our project to increase performance includes identifying activation units on a greater number of randomly generated images, and also trying more combinations of different activation units. This area of research has a significant impact on daily lives, and can help develop areas such as social media, digital marketing, security, journalism, and healthcare. For social media, journalism, and digital marketing, this sort of model makes editing images much easier, providing industry professionals with greater creative freedom. For security purposes, this model can help recreate crime scenes for criminal investigations. Healthcare applications include erasing unnecessary objects from X-rays, MRIs, and other scans in order to focus attention on critical parts.  All in all, this topic of dissecting GAN models to erase and add objects to images is both interesting and important to many present day issues.
</p>
<h2>References</h2>
 
 <p>[1] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba. GAN Dissection: Visualizing and Understanding Generative Adversarial Networks, Proceedings of the International Conference on Learning Representations (ICLR), 2019.
</p>
 

<h2>Team Members</h2>
 <p>1. Riya Gurnani <a href="gurnani.r@northeastern.edu">(gurnani.r@northeastern.edu</a>)</p>
 <p>2. Xinyu Wu <a href="wu.xinyu2@northeastern.edu">(wu.xinyu2@northeastern.edu</a>)</p>

  
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/2024-Spring/">About DS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
