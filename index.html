<!doctype html>
<html lang="en">
<head>
<title>Analysis of GANs</title>
<meta property="og:title" content=Analysis of GANs" />
<meta name="twitter:title" content="Analysis of GANs" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Analysis of GANs</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks"</h2>
<p> By: Riya Gurnani and Xinyu Wu
<p> Paper Link: <a href = "https://openreview.net/pdf?id=Hyg_X2C5FX">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</a></p>
<p> Our Implementation Link: <a href = "https://github.com/rgurn/DS4440_final_project">GitHub (DS4440 Final Project)</a></p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
 <p>
  In this project, we delve into Generative Adversarial Networks (GANs) to better visualize and understand them. “GAN Dissection: Visualizing and Understanding Generative Adversarial Networks” by Bau, Strobelt, et al. introduces a framework for visualizing and understanding GANs at multiple levels of abstraction. The paper provides insights into how GANs represent and generate visual context internally which is crucial given their widespread use in real-world applications. We aim to better understand and expand upon the framework from the paper. Gaining deeper insights into these models will allow for improved performance. Our goal is to successfully implement parts of the code from the paper and test the framework on new data and objects to evaluate the behavior of the model across various datasets and layers. This will also provide further insight into their generalization capabilities. For example, the interactive demo from the paper focused on different church images and the ability to remove/add different objects to the image such as trees, grass, and door. We want to expand on this to test with different types of images (conference room, restaurant, and dining room) and new objects not as tested (window and chair) to understand how well we can apply this concept to find the highest activating units for those objects and be able to generate them in new images.
 </p>

<h2>Paper Review</h2>
<p>
While Generative Adversarial Networks (GANs) have proven significant ability to produce realistic images, there is still a gap in understanding how different knowledge and variants affect model performance. Therefore, this paper looks at the internal representations of GANs and how they are structured, including neurons, objects, and contextual relationships between objects. 
</p>
<p>
Objects that are used in this paper include trees, grass, doors, skies, clouds, bricks, and domes. To examine these objects, the paper utilizes representations to denote tensor outputs (r) on each layer of the generator (G) which generates an image (x) through z layers. In relation to units r, the paper understands it in two phases: (1) dissection, (2) intervention. Dissection involves identifying classes that are explicitly represented in r and measuring the relationships between each single unit and their class, c. The latter, intervention, uses the classes found through dissecting r in order to analyze causal relationships between units and object classes by turning on and off various sets of units. 
</p>
<p>
Building on previous literature, the paper uses an intersection-over-union measure that quantitatively measures the spatial agreement between a unit’s threshold featuremap and a concept’s segmentation. Therefore, this metric is able to rank and pair units with the concepts that most closely match. However, outputs generally depend on multiple parts of the representation, and a combination of units must be identified in relation to a specific object. Thus, an ablation and insertion to the decomposed images can help identify the causal relationship between units and classes, or the average causal effect. This effect can be optimized using L2 regularization and stochastic gradient descent to achieve stronger causal effect.
</p>
<p>
The paper has a number of key findings when understanding units between different datasets, layers, and models: (1) individual unit object detectors form, (2) there are interpretable units for different scene categories such as living room and kitchen, (3) different layers have varying interpretable units, and (4) these interpretable units can help differentiate between GAN models. Overall, the paper provides a strong basis to understanding neurons, objects, and their relationships in GAN models by dissecting and intervening with individual layers. 
</p>
 
<h2>Method</h2>
<p>
 Our first step was to understand the complex framework detailed in the paper and its provided code. It analyzes how objects are encoded within a GAN generator through dissection and intervention. By intervening in the network and observing the effects on generated images, we gain insights into the causal relationships between units and objects. Due to more limited GPU power and some dependency issues, we did not use the entire code from the paper github. Instead, we examined how they created their interactive demo and tutorials to begin. They included the highest activated units for the trees but we needed to find them for our chosen objects of window and chair.
</p>
<p>
 Code from the paper creates the functionality to load an image, click and drag on it to select a certain part which would trigger some action. So, to find the highest activating units for our chosen objects, we utilized parts of this code and added onto it. We wanted to include a variety of testing so we loaded different GAN generators for the objects. For window, we used church and conferenceroom. For chair, we used restaurant and diningroom. Then we created a segment of code to choose one of the generated images and used the select feature to select that object in the image and display the highest activating units. We created a script so we ran this for 50 images for each (window in church, window in conferenceroom, chair in restaurant, and chair in diningroom). Here is an example of what is looks like to find the selected units where the window is selected in the conference room:
</p>
<center><img src="images/get_units_test.png" width="600px" height="300px"></center>
<p>
 We saved all of those selected units and images numbers to 4 files for each. Then we did some analysis on it to find the ten most occurring units for each of the four files. From here, we had four sets of units for our two objects. We then ran testing to see how well those units were selecting and if they could visualize the selected objects in new images and even generate the object when we run the select tool over a part of the image. 
</p>

<h2>Findings</h2>
reports your experimental findings. This should include graphs or images that visualize the results that you have obtained. Ideally this will compare your method to some simpler approach.

<h2>Conclusion</h2>
one paragraph to summarize your conclusions, including any implications for applications or impacts on society, or ideas for future work.

<h2>References</h2>
 
 <p>[1] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba. GAN Dissection: Visualizing and Understanding Generative Adversarial Networks, Proceedings of the International Conference on Learning Representations (ICLR), 2019.
</p>
 

<h2>Team Members</h2>
 <p>1. Riya Gurnani <a href="gurnani.r@northeastern.edu">(gurnani.r@northeastern.edu</a>)</p>
 <p>2. Xinyu Wu <a href="wu.xinyu2@northeastern.edu">(wu.xinyu2@northeastern.edu</a>)</p>

  
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/2024-Spring/">About DS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
