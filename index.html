<!doctype html>
<html lang="en">
<head>
<title>Analysis of GANs</title>
<meta property="og:title" content=Analysis of GANs" />
<meta name="twitter:title" content="Analysis of GANs" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Analysis of GANs</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks"</h2>
<p> By: Riya Gurnani and Xinyu Wu
<p> Paper Link: <a href = "https://openreview.net/pdf?id=Hyg_X2C5FX">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</a></p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
 <p>
  In this project, we delve into Generative Adversarial Networks (GANs) to better visualize and understand them. <q>GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</q> by Bau, Strobelt, et al. (2019) introduces a framework for visualizing and understanding GANs at multiple levels of abstraction. The paper provides insights into how GANs represent and generate visual context internally which is crucial given their widespread use in real-world applications. We aim to better understand and expand upon the framework from the paper. Gaining deeper insights into these models will allow for improved performance. Our goal is to successfully implement parts of the code from the paper and test the framework on new objects to evaluate the behavior of the model across various images. This will also provide further insight into their generalization capabilities. For example, the interactive demo from the paper focused on different church images and the ability to remove/add various objects to the image such as trees, grass, and door. We want to expand on this to test with different types of images (conference room, restaurant, and dining room) and new objects not previously tested (window and chair) to understand how well we can apply this concept to find the highest activating units for those objects and be able to generate them in new images.
 </p>

<h2>Paper Review</h2>
<p>
 While Generative Adversarial Networks (GANs) have shown significant ability to produce realistic images, there is still a gap in understanding how different knowledge and variants affect model performance. Therefore, this paper by Bau et al. looks at the internal representations of GANs and how they are structured, including neurons, objects, and contextual relationships between objects (Bau, Strobelt, et al., 2019). 
</p>
<p>
 Objects that are used in this paper include trees, grass, doors, skies, clouds, bricks, and domes. To examine these objects, the paper utilizes representations to denote tensor outputs (r) on each layer of the generator (G) which generates an image (x) through z layers. In relation to units r, the paper understands it in two phases: (1) dissection, (2) intervention. Dissection involves identifying classes that are explicitly represented in r and measuring the relationships between each single unit and their class, c. The latter, intervention, uses the classes found through dissecting r in order to analyze causal relationships between units and object classes by turning on and off various sets of units. 
</p>
<p>
 Building on previous literature, the paper uses an intersection-over-union measure that quantitatively measures the spatial agreement between a unit&#39;s threshold featuremap and a concept&#39;s segmentation. Therefore, this metric is able to rank and pair units with the concepts that most closely match. However, outputs generally depend on multiple parts of the representation, and a combination of units must be identified in relation to a specific object. Thus, an ablation and insertion to the decomposed images can help identify the causal relationship between units and classes, or the average causal effect. This effect can be optimized using L2 regularization and stochastic gradient descent to achieve stronger causal effect.
</p>
<p>
 The paper has a number of key findings when understanding units between different datasets, layers, and models: (1) individual unit object detectors form, (2) there are interpretable units for different scene categories such as living room and kitchen, (3) different layers have varying interpretable units, and (4) these interpretable units can help differentiate between GAN models. For the purposes of our project, our goal is to identify these interpretable units of objects not tested in this paper, such as windows and chairs, in hopes of replicating similar results. Overall, the paper provides a strong basis to understanding neurons, objects, and their relationships in GAN models by dissecting and intervening with individual layers. 
</p>
 
<h2>Method</h2>
<p> Our Implementation Link: <a href = "https://colab.research.google.com/drive/10UPKkcwzi6PX9RKPq7oID2sOQPjAoJti?usp=sharing">DS4440 Final Project</a></p>
<p>
 In order to achieve our goal of testing different objects, our first step was to understand the complex framework detailed in the paper and its provided code. The code from the paper analyzes how objects are encoded within a GAN generator through dissection and intervention. By intervening in the network and observing the effects on generated images, we gain insights into the causal relationships between units and objects. Due to more limited GPU power and some dependency issues, we did not use the entire code from the paper github. Instead, we examined how they created their interactive demo and tutorials. These pieces of code included the highest activated units for the trees. However, for the purposes of our project, we needed to find the highest activating units for our chosen objects—windows and chairs.
</p>
<p>
 Another piece of code from the paper has the functionality to load an image, click and drag on it to select a certain part in order to trigger either an erasing or adding action. Thus, to find the highest activating units for our own objects, we utilized parts of this code and added onto it. We wanted to include a variety of testing so we loaded different GAN generators for the objects. For reference, here is the first five layers of the pretrained GAN model used that is then trained on the different image settings:
</p>
 <center><img src="images/pretrained_GAN_sample.png" width="300px" height="500px"></center>
 <p>
  For window, we used “church” and “conference room” and for chair, we used “restaurant” and “dining room”. From here, we were able to create a segment of code to randomly choose one of the generated images and used the select feature tool to select that object in the image and display the highest activating units. We created a script to run this for 50 randomly selected images for each object and setting (window in church, window in conference room, chair in restaurant, and chair in dining room). Here is an example of what it looks like to find the selected units where the window is selected in the conference room:
</p>
<center><img src="images/get_units_test.png" width="600px" height="300px"></center>
 <p>
  We saved all of those selected units and images numbers to four files corresponding to each object-setting combination. Then, we did some analysis on this data to find the ten most commonly occurring units for each of the four object-setting combinations. Therefore, we had four sets of units across the two objects. Using these most commonly activated units, we then ran testing to see how well those units were selecting the objects and if they could visualize the selected objects in new images and even generate the object when we run the select tool over a part of the image. 
 </p>

<h2>Findings</h2>
reports your experimental findings. This should include graphs or images that visualize the results that you have obtained. Ideally this will compare your method to some simpler approach.

<h2>Conclusion</h2>
one paragraph to summarize your conclusions, including any implications for applications or impacts on society, or ideas for future work.

<h2>References</h2>
 
 <p>[1] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba. GAN Dissection: Visualizing and Understanding Generative Adversarial Networks, Proceedings of the International Conference on Learning Representations (ICLR), 2019.
</p>
 

<h2>Team Members</h2>
 <p>1. Riya Gurnani <a href="gurnani.r@northeastern.edu">(gurnani.r@northeastern.edu</a>)</p>
 <p>2. Xinyu Wu <a href="wu.xinyu2@northeastern.edu">(wu.xinyu2@northeastern.edu</a>)</p>

  
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/2024-Spring/">About DS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
